{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyse de sentiments sur les films du TOP 100 box office (Notebook pour le processing des données) 2/3**\n",
    "**Projet Python - 2A ENSAE**\n",
    "\n",
    "AUMONT Louis-Armand, KHAIRALDIN Ahmed, GIMENES Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce Notebook va être de charger la base de données, puis nettoyer les commentaires pour les \"tokenisés\" et enfin les \"lemmatisés\"\n",
    "et appliquer des algorithmes de NLP sur nos commentaires \"lemmatisés\" afin de déduire pour chaque commentaire un score de polarité et afin former un nouveau dataset final où chaque observation sera un film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de pouvoir faire de l'analyse de sentiment pour chaque commentaire, il faut d'abord les nettoyer pour les rendre lisibles pour l'ordinateur. Premièrement, nous allons transformer le fichier data_reviews en une base de données où chaque observation est un commentaire. Puis, nous allons nettoyer chaque commentaire en utilisant des fonctions permettant de supprimer la ponctuation, les stop-words et la lemmatisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Download Library ⚙️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/mamba/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/mamba/lib/python3.10/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/mamba/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from spacy) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/mamba/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: wordcloud in /opt/mamba/lib/python3.10/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (1.26.2)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: textblob in /opt/mamba/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/mamba/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut également télécharger une fois ces sortes de sous packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Téléchargement d'éléments nécessaires à la tokenisation\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/mamba/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/mamba/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import Library 📦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de packages nécessaires\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Création de la base de données des commentaires, tokenisation et lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Création du DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ouvrir le dataset de telle façon qu'une observation est un commentaire sur un film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du fichier json en un Dataframe Pandas\n",
    "\n",
    "# Charger le fichier JSON\n",
    "with open(\"data_reviews_1o2.json\", 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Film</th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Annee</th>\n",
       "      <th>Note imdb</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box office</th>\n",
       "      <th>duree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17471</th>\n",
       "      <td>Les Animaux fantastiques (2016)</td>\n",
       "      <td>Many reviews here are from Harry Potter fans w...</td>\n",
       "      <td>2016</td>\n",
       "      <td>7,2</td>\n",
       "      <td>180000000</td>\n",
       "      <td>816037575</td>\n",
       "      <td>2h 12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16437</th>\n",
       "      <td>Inception (2010)</td>\n",
       "      <td>Not much more to add really. I managed to watc...</td>\n",
       "      <td>2010</td>\n",
       "      <td>8,8</td>\n",
       "      <td>160000000</td>\n",
       "      <td>839030630</td>\n",
       "      <td>2h 28m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>Le Seigneur des anneaux : Le Retour du roi (2003)</td>\n",
       "      <td>This movie could really use an EE -- BADLY. Ch...</td>\n",
       "      <td>2003</td>\n",
       "      <td>9,0</td>\n",
       "      <td>94000000</td>\n",
       "      <td>1155870721</td>\n",
       "      <td>3h 21m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>Star Wars : Épisode VIII - Les Derniers Jedi (...</td>\n",
       "      <td>For film fanatics like myself, The Last Jedi i...</td>\n",
       "      <td>2017</td>\n",
       "      <td>6,9</td>\n",
       "      <td>317000000</td>\n",
       "      <td>1334407706</td>\n",
       "      <td>2h 32m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15095</th>\n",
       "      <td>Black Panther: Wakanda Forever (2022)</td>\n",
       "      <td>As the last film in the fourth phase of the Ma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>6,7</td>\n",
       "      <td>250000000</td>\n",
       "      <td>859208836</td>\n",
       "      <td>2h 41m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Film  \\\n",
       "17471                    Les Animaux fantastiques (2016)   \n",
       "16437                                   Inception (2010)   \n",
       "4959   Le Seigneur des anneaux : Le Retour du roi (2003)   \n",
       "3362   Star Wars : Épisode VIII - Les Derniers Jedi (...   \n",
       "15095              Black Panther: Wakanda Forever (2022)   \n",
       "\n",
       "                                             Commentaire Annee Note imdb  \\\n",
       "17471  Many reviews here are from Harry Potter fans w...  2016       7,2   \n",
       "16437  Not much more to add really. I managed to watc...  2010       8,8   \n",
       "4959   This movie could really use an EE -- BADLY. Ch...  2003       9,0   \n",
       "3362   For film fanatics like myself, The Last Jedi i...  2017       6,9   \n",
       "15095  As the last film in the fourth phase of the Ma...  2022       6,7   \n",
       "\n",
       "          Budget  Box office   duree  \n",
       "17471  180000000   816037575  2h 12m  \n",
       "16437  160000000   839030630  2h 28m  \n",
       "4959    94000000  1155870721  3h 21m  \n",
       "3362   317000000  1334407706  2h 32m  \n",
       "15095  250000000   859208836  2h 41m  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name = []\n",
    "comments = []\n",
    "notes = []\n",
    "year= []\n",
    "budget=[]\n",
    "recette=[]\n",
    "duree=[]\n",
    "\n",
    "for movie, dico in data.items():\n",
    "    if '0' in dico and isinstance(dico['0'], list):  # Vérifier si la clé '0' est une liste\n",
    "        for comment in dico['0']:\n",
    "            movie_name.append(movie)\n",
    "            comments.append(comment)\n",
    "            year.append(dico['1'])\n",
    "            notes.append(dico['2'])\n",
    "            budget.append(dico['3'])\n",
    "            recette.append(dico['4'])\n",
    "            duree.append(dico['5'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Film': movie_name,\n",
    "    'Commentaire': comments,\n",
    "    'Annee':year,\n",
    "    'Note imdb': notes,\n",
    "    'Budget':budget,\n",
    "    'Box office':recette,\n",
    "    'duree':duree\n",
    "})\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Nettoyage des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.1** Tokenisation et suppression de la ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut executer la cellule ci-dessous pour pouvoir charger le modèle de la langue anglaise de scapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètre permettant d'afficher un commentaire même si il est très long\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17860    \"Shrek the Third\" is funny and has big laughs throughout, and one gets the feel that this installment of the big green guy's story is completely unnecessary. The cryptic symptom of Shrek's demise is somewhat insidious. All the big laughs come from the supporting characters namely Pinocchio (Cody Cameron) and the Ginger Bread Man (Conrad Vernon). Justin Timberlake voices a great turn as Artie, heir to Far Far Away Throne. He has a great scene where he says, \"Sometimes we all get in our own way\n",
       "\" This wonderful sentiment is so misplaced in this feeble attempt to empower faith in oneself. Also the movie should have heeded its own advice. \"Shrek\" the first was endearing in its message of seeing beyond appearances and finding the value of a person's or ogre's soul. \"Shrek\" also had a great sense of humor about life and itself. Apparently, the makers of \"Shrek the Third\" have forgotten this legacy. And there are too many involved. Chris Miller is director along with co-director Raman Hui. Still based on the original book by William Steig, there are a horde of writers: Jeff Price, Peter Seaman, J. David Stern, David Weiss, and Jon Zack. Consequently, \"Shrek the Third\" is blurred. The movie is funny; however, the laughs are of the sitcom nature. Some of the gags themselves are bewildering. For example, the extended death scene of King Harold (John Cleese) gets old until he finally \"croaks\", so to speak. The playing of Paul McCartney and Wings' \"Live and Let Die\" at the funeral is out of place and irrelevant. Also who is the main audience of \"Shrek 3\"? Old people like me or kids? Kids don't know Wings, much less The Beatles. \"Shrek the Third\" is perfunctory funny, but the fairy tale well has gone dry. Rather the writers did not create something fresh, instead polishing a retread.As \"Shrek the Third\" opens we find Prince Charming (very funny Rupert Everett) doing very bad dinner theater. Shrek (Mike Myers) and Finona (Cameron Diaz) are filling in for the royal duties of Far Far Away Kingdom, and not really having a good time. So when the King finally dies, Shrek is next in line to the Throne. Unless Shrek can convince Prince Artie (Justin Timberlake) to assume the kingly duties, he will be unable to return to his beloved swamp. Thus, Shrek along with sidekicks Donkey (Eddie Murphy) and Puss In Boots (Antonio Banderas) embark on mission to retrieve young Artie. Meanwhile, Fiona and mother Queen Lillian (Julie Andrews) remain in Far Far Away. Fiona also confesses to Shrek before he leaves, that he will soon be a father. Shrek outwardly does not assume the tower of fatherhood. A ridiculous subplot involves Prince Charming's claim to the Throne. High jinx abound, and there are some touching heart to heart conversations between Shrek and Artie. Unfortunately, nothing really special distinguishes this third go round of the lovable Ogre. The supporting characters seem to corner the market on all the funny lines. Mike Myers, Eddie Murphy, and Antonio Banderas are all great as Shrek, Donkey, and Puss. Cameron Diaz seems relegated to the sidelines in this outing. Rupert Everett provides comic relief as the totally self absorbed and clueless Prince Charming. Perhaps, the movie's only freshness comes from Justin Timberlake as Artie.Other than Timberlake's Artie, \"Shrek the Third\" really shows signs of wear and tear. Here \"Shrek 3\" even eludes its connection to pop culture. There is a forced \"hip-hop\" conversation between Shrek and Artie. \"Shrek the Third\" suffers from a stand alone gag structure. There is no coherence. Perhaps, all of the charming and sweet fairy tale essence has been weaned dry. The message near the end about believing in oneself is noble, but seems tacked on and out of context. \"Shrek the Third\" is over wrought, funny, and really not worth seeing. Perhaps, \"Shrek 4\" will recapture some of what made it so special. Then again, it may be time to move on to something new.\n",
       "Name: Commentaire, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On retire les \"\\n\" dans les commentaires\n",
    "df[\"Commentaire\"] = df[\"Commentaire\"].str.replace('\\n','')\n",
    "df[\"Commentaire\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence par charger le modèle de la langue anglaise de Spacy (les commentaires sont en anglais)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Fonction qui prend en argument un commentaire (chaîne de caractère) et qui retourne une liste des tokens sans les stopwords et la ponctuation\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 44s, sys: 7.02 s, total: 25min 51s\n",
      "Wall time: 25min 59s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Film</th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Annee</th>\n",
       "      <th>Note imdb</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box office</th>\n",
       "      <th>duree</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>Le Hobbit: La Désolation de Smaug (2013)</td>\n",
       "      <td>This must be the best movie of 2013. There is no movie that comes close to it recently. I must s...</td>\n",
       "      <td>2013</td>\n",
       "      <td>7,8</td>\n",
       "      <td>225000000</td>\n",
       "      <td>959027992</td>\n",
       "      <td>2h 41m</td>\n",
       "      <td>[best, movie, 2013, movie, comes, close, recently, enjoyed, enjoy, second, far, experience, seei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>Barbie (2023)</td>\n",
       "      <td>\"I'm just so tired of watching myself and every single other woman tie herself into knots so tha...</td>\n",
       "      <td>2023</td>\n",
       "      <td>7,0</td>\n",
       "      <td>100000000</td>\n",
       "      <td>1441788910</td>\n",
       "      <td>1h 54m</td>\n",
       "      <td>[tired, watching, single, woman, tie, knots, people, like, true, doll, \"Bro, excited, look, beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>Aquaman (2018)</td>\n",
       "      <td>I was proper looking forward to this fish man hybrid film but wow just wow, how did they get thi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>6,8</td>\n",
       "      <td>160000000</td>\n",
       "      <td>1157347433</td>\n",
       "      <td>2h 23m</td>\n",
       "      <td>[proper, looking, forward, fish, man, hybrid, film, wow, wow, wrong, watch, tin, paint, drying, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12028</th>\n",
       "      <td>Les Minions 2: Il était une fois Gru (2022)</td>\n",
       "      <td>I cannot express my love for this beautiful masterpiece of art. This brought me to tears, howlin...</td>\n",
       "      <td>2022</td>\n",
       "      <td>6,5</td>\n",
       "      <td>80000000</td>\n",
       "      <td>939628210</td>\n",
       "      <td>1h 27m</td>\n",
       "      <td>[express, love, beautiful, masterpiece, art, brought, tears, howling, laughter, depression, fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>Jumanji : Bienvenue dans la jungle (2017)</td>\n",
       "      <td>A modern twist provides a decent afternoon filler, end climax scene might be a bit OTT, but film...</td>\n",
       "      <td>2017</td>\n",
       "      <td>6,9</td>\n",
       "      <td>90000000</td>\n",
       "      <td>995339117</td>\n",
       "      <td>1h 59m</td>\n",
       "      <td>[modern, twist, provides, decent, afternoon, filler, end, climax, scene, bit, OTT, film, watcher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Film  \\\n",
       "11108     Le Hobbit: La Désolation de Smaug (2013)   \n",
       "2550                                 Barbie (2023)   \n",
       "5240                                Aquaman (2018)   \n",
       "12028  Les Minions 2: Il était une fois Gru (2022)   \n",
       "9924     Jumanji : Bienvenue dans la jungle (2017)   \n",
       "\n",
       "                                                                                               Commentaire  \\\n",
       "11108  This must be the best movie of 2013. There is no movie that comes close to it recently. I must s...   \n",
       "2550   \"I'm just so tired of watching myself and every single other woman tie herself into knots so tha...   \n",
       "5240   I was proper looking forward to this fish man hybrid film but wow just wow, how did they get thi...   \n",
       "12028  I cannot express my love for this beautiful masterpiece of art. This brought me to tears, howlin...   \n",
       "9924   A modern twist provides a decent afternoon filler, end climax scene might be a bit OTT, but film...   \n",
       "\n",
       "      Annee Note imdb     Budget  Box office   duree  \\\n",
       "11108  2013       7,8  225000000   959027992  2h 41m   \n",
       "2550   2023       7,0  100000000  1441788910  1h 54m   \n",
       "5240   2018       6,8  160000000  1157347433  2h 23m   \n",
       "12028  2022       6,5   80000000   939628210  1h 27m   \n",
       "9924   2017       6,9   90000000   995339117  1h 59m   \n",
       "\n",
       "                                                                                                    Tokens  \n",
       "11108  [best, movie, 2013, movie, comes, close, recently, enjoyed, enjoy, second, far, experience, seei...  \n",
       "2550   [tired, watching, single, woman, tie, knots, people, like, true, doll, \"Bro, excited, look, beau...  \n",
       "5240   [proper, looking, forward, fish, man, hybrid, film, wow, wow, wrong, watch, tin, paint, drying, ...  \n",
       "12028  [express, love, beautiful, masterpiece, art, brought, tears, howling, laughter, depression, fina...  \n",
       "9924   [modern, twist, provides, decent, afternoon, filler, end, climax, scene, bit, OTT, film, watcher...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# On crée une nouvelle colonne avec la liste des tokens pour chaque commentaire\n",
    "df['Tokens'] = df['Commentaire'].apply(tokenize)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Spacy, l'algortihme s'exécute en près de  min pour l'ensemble de la base de données. Nous avons utilisé Spacy et non nltk pour la tokenization et la suppression des stopwords, puisque Spacy a un répertoire plus important de stopwords et a de meilleures performances pour les textes volumineux. Il est à noter que la fonction de tokenisation ne supprime pas les négations, donc l'analyse de sentiment ne sera pas biaisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.2** Lemmatisation des commentaires   \n",
    "Nous allons désormais procéder à la lemmatisation des commentaires pour pouvoir allèger les algorithmes de NLP plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui prend en argument une liste de tokens et qui retourne ces tokens lemmatisés\n",
    "def lemm(tokens):\n",
    "    # D'abord, on transforme la liste en doc Spacy\n",
    "    tokens_as_doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "    # Lemmatisation du doc \n",
    "    lemmatized = [token.lemma_ for token in tokens_as_doc]\n",
    "    return lemmatized\n",
    "# Le lemma_ de Spacy ne reconnait pas les tokens et renvoie des listes vides (Pourquoi ?)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm2(tokens):\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une nouvelle colonne des tokens lemmatisés\n",
    "df['Tokens lemmatisés'] = df['Tokens'].apply(lemm2)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Représentation des mots\n",
    "\n",
    "Maintenant, visualisons pour certains commentaires les mots les plus représentés après nettoyage des commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons d'abord par visualiser un premier nuage de mots d'un commentaire quelconque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui prend en argument une liste de tokens et qui retourne le nuage de mots correspondant.\n",
    "\n",
    "def cloud(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cloud(df['Tokens lemmatisés'][0]), interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, pour le premier commentaire de la bdd, on peut dès à présent avoir une idée sur son avis vis à vis du film grâce à ce nuage de mots, de par la présence de mots tels que 'masterpiece, 'magnificent', 'perfection'... qui sont assez représentés. Néanmoins, comme nous avons utilisé nltk, le lemmatiseur ne reconnait pas les entités nommés telles que 'IMAX', 'India'... . \n",
    "La présence des mots concernants le temps comme \"waiting\", \"time\", \"longest\" nous indique que la durée du film est un sujet des commentaires et nous pourrons voir par la suite s'il existe une corrélation entre la durée du film et la polarité des commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** NLP et analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps nous allons appliquer les algorithmes de NLP sur les commentaires afin d'obtenir un score pour chaque commentaires puis nous allons modifier le dataset de telle sorte qu'une observation est un film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1** application d'algorithmes d'analyses de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertit les commentaires en string.\n",
    "def string_function(column):\n",
    "    return column.str.lower()\n",
    "\n",
    "df['Commentaire'] =  string_function(df[\"Commentaire\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons comparer les différents algorithmes proposés, notamment ceux des modules TextBlob, nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va dans un premier temps utiliser le module TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_blob(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applique la fonction à la colonne 'tokens_lemmatized' du DataFrame\n",
    "df['sentiment_polarity_blob'] = df['Tokens lemmatisés'].apply(analyze_sentiment_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaye maintenant avec la fonction SentimentIntensityAnalyzer de nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_nltk(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['sentiment_polarity_nltk'] = df[\"Tokens lemmatisés\"].apply(analyze_sentiment_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessus prend environ 2 min à s'exécuter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2** manipulation pour la création du nouveau dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables sont sous forme de string. Nous changeons cela pour pourvoir faire des statistiques avec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change le type de la colonne 'Note imdb' en float\n",
    "df['Note imdb'] = df['Note imdb'].str.replace(',', '.').astype(float)\n",
    "df['Annee'] = df['Annee'].astype(float)\n",
    "df[\"Budget\"] = df[\"Budget\"].astype(float)\n",
    "df['Box office'] = df[\"Box office\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On groupe les observations selon le film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('Film')\n",
    "df_grouped = grouped[[\"sentiment_polarity_nltk\", \"sentiment_polarity_blob\",  'Note imdb', 'Annee', \"Budget\", \"Box office\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intéressons nous à la corrélation des notes avec les scores moyens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va également ajouter la durée de chaque film dans le nouveau df \"df_grouped\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une fonction pour sélectionner une observation pour chaque groupe\n",
    "def select_observation(group):\n",
    "    # Vous pouvez personnaliser cette fonction pour choisir une observation spécifique,\n",
    "    # par exemple, ici, nous choisissons la première observation de chaque groupe.\n",
    "    return group.iloc[0]\n",
    "\n",
    "# Appliquer la fonction à chaque groupe\n",
    "duration_serie = grouped['duree'].apply(select_observation)\n",
    "duration_serie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(duration_serie, df_grouped , on='Film', how='inner')\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il peut être interessant de regarder aussi la variance des scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_by_group = grouped[[\"sentiment_polarity_nltk\", \"sentiment_polarity_blob\"]].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Join\" var_by_group et le df pour ajouter les variables de variance\n",
    "df_grouped_2 = pd.merge(var_by_group, df_grouped , on='Film', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Renomme les variables obtenues:\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_nltk_x': 'var_score_nltk'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_blob_x': 'var_score_blob'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_nltk_y': 'sentiment_polarity_nltk'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_blob_y': 'sentiment_polarity_blob'}, inplace=True)\n",
    "\n",
    "# Rétablir l'index par défaut\n",
    "df_grouped_2.reset_index(inplace=True)\n",
    "print(df_grouped_2.shape)\n",
    "df_grouped_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_2.to_csv('data_post_process_2o2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_2 = pd.read_csv('data_post_process_2o2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérification\n",
    "df_grouped_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos data ont été traité, nous allons procéder à l'analyse statistique sur un autre notebook (\"Analyse_modelisation_3o3\") avec les \"data_post_process_2o2\" afin de faciliter la lecture du projet. En effet, l'exécution de ce notebook peut prendre une quinzaine de minute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
