{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyse de sentiments sur les films du TOP 100 box office (Notebook pour le processing des donn√©es) 2/3**\n",
    "**Projet Python - 2A ENSAE**\n",
    "\n",
    "AUMONT Louis-Armand, KHAIRALDIN Ahmed, GIMENES Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce Notebook va √™tre de charger la base de donn√©es, puis nettoyer les commentaires pour les \"tokenis√©s\" et enfin les \"lemmatis√©s\"\n",
    "et appliquer des algorithmes de NLP sur nos commentaires \"lemmatis√©s\" afin de d√©duire pour chaque commentaire un score de polarit√© et afin former un nouveau dataset final o√π chaque observation sera un film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de pouvoir faire de l'analyse de sentiment pour chaque commentaire, il faut d'abord les nettoyer pour les rendre lisibles pour l'ordinateur. Premi√®rement, nous allons transformer le fichier data_reviews en une base de donn√©es o√π chaque observation est un commentaire. Puis, nous allons nettoyer chaque commentaire en utilisant des fonctions permettant de supprimer la ponctuation, les stop-words et la lemmatisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Download Library ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/mamba/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/mamba/lib/python3.10/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/mamba/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/mamba/lib/python3.10/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from spacy) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/mamba/lib/python3.10/site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/mamba/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: wordcloud in /opt/mamba/lib/python3.10/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (1.26.2)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: textblob in /opt/mamba/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/mamba/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut √©galement t√©l√©charger une fois ces sortes de sous packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T√©l√©chargement d'√©l√©ments n√©cessaires √† la tokenisation\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/mamba/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/mamba/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/mamba/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/mamba/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/mamba/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/mamba/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import Library üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de packages n√©cessaires\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Cr√©ation de la base de donn√©es des commentaires, tokenisation et lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Cr√©ation du DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ouvrir le dataset de telle fa√ßon qu'une observation est un commentaire sur un film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du fichier json en un Dataframe Pandas\n",
    "\n",
    "# Charger le fichier JSON\n",
    "with open(\"data_reviews_1o2.json\", 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Film</th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Annee</th>\n",
       "      <th>Note imdb</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box office</th>\n",
       "      <th>duree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17471</th>\n",
       "      <td>Les Animaux fantastiques (2016)</td>\n",
       "      <td>Many reviews here are from Harry Potter fans w...</td>\n",
       "      <td>2016</td>\n",
       "      <td>7,2</td>\n",
       "      <td>180000000</td>\n",
       "      <td>816037575</td>\n",
       "      <td>2h 12m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16437</th>\n",
       "      <td>Inception (2010)</td>\n",
       "      <td>Not much more to add really. I managed to watc...</td>\n",
       "      <td>2010</td>\n",
       "      <td>8,8</td>\n",
       "      <td>160000000</td>\n",
       "      <td>839030630</td>\n",
       "      <td>2h 28m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>Le Seigneur des anneaux : Le Retour du roi (2003)</td>\n",
       "      <td>This movie could really use an EE -- BADLY. Ch...</td>\n",
       "      <td>2003</td>\n",
       "      <td>9,0</td>\n",
       "      <td>94000000</td>\n",
       "      <td>1155870721</td>\n",
       "      <td>3h 21m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>Star Wars : √âpisode VIII - Les Derniers Jedi (...</td>\n",
       "      <td>For film fanatics like myself, The Last Jedi i...</td>\n",
       "      <td>2017</td>\n",
       "      <td>6,9</td>\n",
       "      <td>317000000</td>\n",
       "      <td>1334407706</td>\n",
       "      <td>2h 32m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15095</th>\n",
       "      <td>Black Panther: Wakanda Forever (2022)</td>\n",
       "      <td>As the last film in the fourth phase of the Ma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>6,7</td>\n",
       "      <td>250000000</td>\n",
       "      <td>859208836</td>\n",
       "      <td>2h 41m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Film  \\\n",
       "17471                    Les Animaux fantastiques (2016)   \n",
       "16437                                   Inception (2010)   \n",
       "4959   Le Seigneur des anneaux : Le Retour du roi (2003)   \n",
       "3362   Star Wars : √âpisode VIII - Les Derniers Jedi (...   \n",
       "15095              Black Panther: Wakanda Forever (2022)   \n",
       "\n",
       "                                             Commentaire Annee Note imdb  \\\n",
       "17471  Many reviews here are from Harry Potter fans w...  2016       7,2   \n",
       "16437  Not much more to add really. I managed to watc...  2010       8,8   \n",
       "4959   This movie could really use an EE -- BADLY. Ch...  2003       9,0   \n",
       "3362   For film fanatics like myself, The Last Jedi i...  2017       6,9   \n",
       "15095  As the last film in the fourth phase of the Ma...  2022       6,7   \n",
       "\n",
       "          Budget  Box office   duree  \n",
       "17471  180000000   816037575  2h 12m  \n",
       "16437  160000000   839030630  2h 28m  \n",
       "4959    94000000  1155870721  3h 21m  \n",
       "3362   317000000  1334407706  2h 32m  \n",
       "15095  250000000   859208836  2h 41m  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name = []\n",
    "comments = []\n",
    "notes = []\n",
    "year= []\n",
    "budget=[]\n",
    "recette=[]\n",
    "duree=[]\n",
    "\n",
    "for movie, dico in data.items():\n",
    "    if '0' in dico and isinstance(dico['0'], list):  # V√©rifier si la cl√© '0' est une liste\n",
    "        for comment in dico['0']:\n",
    "            movie_name.append(movie)\n",
    "            comments.append(comment)\n",
    "            year.append(dico['1'])\n",
    "            notes.append(dico['2'])\n",
    "            budget.append(dico['3'])\n",
    "            recette.append(dico['4'])\n",
    "            duree.append(dico['5'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Film': movie_name,\n",
    "    'Commentaire': comments,\n",
    "    'Annee':year,\n",
    "    'Note imdb': notes,\n",
    "    'Budget':budget,\n",
    "    'Box office':recette,\n",
    "    'duree':duree\n",
    "})\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Nettoyage des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.1** Tokenisation et suppression de la ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut executer la cellule ci-dessous pour pouvoir charger le mod√®le de la langue anglaise de scapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tre permettant d'afficher un commentaire m√™me si il est tr√®s long\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17860    \"Shrek the Third\" is funny and has big laughs throughout, and one gets the feel that this installment of the big green guy's story is completely unnecessary. The cryptic symptom of Shrek's demise is somewhat insidious. All the big laughs come from the supporting characters namely Pinocchio (Cody Cameron) and the Ginger Bread Man (Conrad Vernon). Justin Timberlake voices a great turn as Artie, heir to Far Far Away Throne. He has a great scene where he says, \"Sometimes we all get in our own way\n",
       "\" This wonderful sentiment is so misplaced in this feeble attempt to empower faith in oneself. Also the movie should have heeded its own advice. \"Shrek\" the first was endearing in its message of seeing beyond appearances and finding the value of a person's or ogre's soul. \"Shrek\" also had a great sense of humor about life and itself. Apparently, the makers of \"Shrek the Third\" have forgotten this legacy. And there are too many involved. Chris Miller is director along with co-director Raman Hui. Still based on the original book by William Steig, there are a horde of writers: Jeff Price, Peter Seaman, J. David Stern, David Weiss, and Jon Zack. Consequently, \"Shrek the Third\" is blurred. The movie is funny; however, the laughs are of the sitcom nature. Some of the gags themselves are bewildering. For example, the extended death scene of King Harold (John Cleese) gets old until he finally \"croaks\", so to speak. The playing of Paul McCartney and Wings' \"Live and Let Die\" at the funeral is out of place and irrelevant. Also who is the main audience of \"Shrek 3\"? Old people like me or kids? Kids don't know Wings, much less The Beatles. \"Shrek the Third\" is perfunctory funny, but the fairy tale well has gone dry. Rather the writers did not create something fresh, instead polishing a retread.As \"Shrek the Third\" opens we find Prince Charming (very funny Rupert Everett) doing very bad dinner theater. Shrek (Mike Myers) and Finona (Cameron Diaz) are filling in for the royal duties of Far Far Away Kingdom, and not really having a good time. So when the King finally dies, Shrek is next in line to the Throne. Unless Shrek can convince Prince Artie (Justin Timberlake) to assume the kingly duties, he will be unable to return to his beloved swamp. Thus, Shrek along with sidekicks Donkey (Eddie Murphy) and Puss In Boots (Antonio Banderas) embark on mission to retrieve young Artie. Meanwhile, Fiona and mother Queen Lillian (Julie Andrews) remain in Far Far Away. Fiona also confesses to Shrek before he leaves, that he will soon be a father. Shrek outwardly does not assume the tower of fatherhood. A ridiculous subplot involves Prince Charming's claim to the Throne. High jinx abound, and there are some touching heart to heart conversations between Shrek and Artie. Unfortunately, nothing really special distinguishes this third go round of the lovable Ogre. The supporting characters seem to corner the market on all the funny lines. Mike Myers, Eddie Murphy, and Antonio Banderas are all great as Shrek, Donkey, and Puss. Cameron Diaz seems relegated to the sidelines in this outing. Rupert Everett provides comic relief as the totally self absorbed and clueless Prince Charming. Perhaps, the movie's only freshness comes from Justin Timberlake as Artie.Other than Timberlake's Artie, \"Shrek the Third\" really shows signs of wear and tear. Here \"Shrek 3\" even eludes its connection to pop culture. There is a forced \"hip-hop\" conversation between Shrek and Artie. \"Shrek the Third\" suffers from a stand alone gag structure. There is no coherence. Perhaps, all of the charming and sweet fairy tale essence has been weaned dry. The message near the end about believing in oneself is noble, but seems tacked on and out of context. \"Shrek the Third\" is over wrought, funny, and really not worth seeing. Perhaps, \"Shrek 4\" will recapture some of what made it so special. Then again, it may be time to move on to something new.\n",
       "Name: Commentaire, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On retire les \"\\n\" dans les commentaires\n",
    "df[\"Commentaire\"] = df[\"Commentaire\"].str.replace('\\n','')\n",
    "df[\"Commentaire\"].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence par charger le mod√®le de la langue anglaise de Spacy (les commentaires sont en anglais)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Fonction qui prend en argument un commentaire (cha√Æne de caract√®re) et qui retourne une liste des tokens sans les stopwords et la ponctuation\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 44s, sys: 7.02 s, total: 25min 51s\n",
      "Wall time: 25min 59s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Film</th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Annee</th>\n",
       "      <th>Note imdb</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Box office</th>\n",
       "      <th>duree</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>Le Hobbit: La D√©solation de Smaug (2013)</td>\n",
       "      <td>This must be the best movie of 2013. There is no movie that comes close to it recently. I must s...</td>\n",
       "      <td>2013</td>\n",
       "      <td>7,8</td>\n",
       "      <td>225000000</td>\n",
       "      <td>959027992</td>\n",
       "      <td>2h 41m</td>\n",
       "      <td>[best, movie, 2013, movie, comes, close, recently, enjoyed, enjoy, second, far, experience, seei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>Barbie (2023)</td>\n",
       "      <td>\"I'm just so tired of watching myself and every single other woman tie herself into knots so tha...</td>\n",
       "      <td>2023</td>\n",
       "      <td>7,0</td>\n",
       "      <td>100000000</td>\n",
       "      <td>1441788910</td>\n",
       "      <td>1h 54m</td>\n",
       "      <td>[tired, watching, single, woman, tie, knots, people, like, true, doll, \"Bro, excited, look, beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>Aquaman (2018)</td>\n",
       "      <td>I was proper looking forward to this fish man hybrid film but wow just wow, how did they get thi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>6,8</td>\n",
       "      <td>160000000</td>\n",
       "      <td>1157347433</td>\n",
       "      <td>2h 23m</td>\n",
       "      <td>[proper, looking, forward, fish, man, hybrid, film, wow, wow, wrong, watch, tin, paint, drying, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12028</th>\n",
       "      <td>Les Minions 2: Il √©tait une fois Gru (2022)</td>\n",
       "      <td>I cannot express my love for this beautiful masterpiece of art. This brought me to tears, howlin...</td>\n",
       "      <td>2022</td>\n",
       "      <td>6,5</td>\n",
       "      <td>80000000</td>\n",
       "      <td>939628210</td>\n",
       "      <td>1h 27m</td>\n",
       "      <td>[express, love, beautiful, masterpiece, art, brought, tears, howling, laughter, depression, fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>Jumanji : Bienvenue dans la jungle (2017)</td>\n",
       "      <td>A modern twist provides a decent afternoon filler, end climax scene might be a bit OTT, but film...</td>\n",
       "      <td>2017</td>\n",
       "      <td>6,9</td>\n",
       "      <td>90000000</td>\n",
       "      <td>995339117</td>\n",
       "      <td>1h 59m</td>\n",
       "      <td>[modern, twist, provides, decent, afternoon, filler, end, climax, scene, bit, OTT, film, watcher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Film  \\\n",
       "11108     Le Hobbit: La D√©solation de Smaug (2013)   \n",
       "2550                                 Barbie (2023)   \n",
       "5240                                Aquaman (2018)   \n",
       "12028  Les Minions 2: Il √©tait une fois Gru (2022)   \n",
       "9924     Jumanji : Bienvenue dans la jungle (2017)   \n",
       "\n",
       "                                                                                               Commentaire  \\\n",
       "11108  This must be the best movie of 2013. There is no movie that comes close to it recently. I must s...   \n",
       "2550   \"I'm just so tired of watching myself and every single other woman tie herself into knots so tha...   \n",
       "5240   I was proper looking forward to this fish man hybrid film but wow just wow, how did they get thi...   \n",
       "12028  I cannot express my love for this beautiful masterpiece of art. This brought me to tears, howlin...   \n",
       "9924   A modern twist provides a decent afternoon filler, end climax scene might be a bit OTT, but film...   \n",
       "\n",
       "      Annee Note imdb     Budget  Box office   duree  \\\n",
       "11108  2013       7,8  225000000   959027992  2h 41m   \n",
       "2550   2023       7,0  100000000  1441788910  1h 54m   \n",
       "5240   2018       6,8  160000000  1157347433  2h 23m   \n",
       "12028  2022       6,5   80000000   939628210  1h 27m   \n",
       "9924   2017       6,9   90000000   995339117  1h 59m   \n",
       "\n",
       "                                                                                                    Tokens  \n",
       "11108  [best, movie, 2013, movie, comes, close, recently, enjoyed, enjoy, second, far, experience, seei...  \n",
       "2550   [tired, watching, single, woman, tie, knots, people, like, true, doll, \"Bro, excited, look, beau...  \n",
       "5240   [proper, looking, forward, fish, man, hybrid, film, wow, wow, wrong, watch, tin, paint, drying, ...  \n",
       "12028  [express, love, beautiful, masterpiece, art, brought, tears, howling, laughter, depression, fina...  \n",
       "9924   [modern, twist, provides, decent, afternoon, filler, end, climax, scene, bit, OTT, film, watcher...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# On cr√©e une nouvelle colonne avec la liste des tokens pour chaque commentaire\n",
    "df['Tokens'] = df['Commentaire'].apply(tokenize)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Spacy, l'algortihme s'ex√©cute en pr√®s de  min pour l'ensemble de la base de donn√©es. Nous avons utilis√© Spacy et non nltk pour la tokenization et la suppression des stopwords, puisque Spacy a un r√©pertoire plus important de stopwords et a de meilleures performances pour les textes volumineux. Il est √† noter que la fonction de tokenisation ne supprime pas les n√©gations, donc l'analyse de sentiment ne sera pas biais√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.2** Lemmatisation des commentaires   \n",
    "Nous allons d√©sormais proc√©der √† la lemmatisation des commentaires pour pouvoir all√®ger les algorithmes de NLP plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui prend en argument une liste de tokens et qui retourne ces tokens lemmatis√©s\n",
    "def lemm(tokens):\n",
    "    # D'abord, on transforme la liste en doc Spacy\n",
    "    tokens_as_doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "    # Lemmatisation du doc \n",
    "    lemmatized = [token.lemma_ for token in tokens_as_doc]\n",
    "    return lemmatized\n",
    "# Le lemma_ de Spacy ne reconnait pas les tokens et renvoie des listes vides (Pourquoi ?)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm2(tokens):\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e une nouvelle colonne des tokens lemmatis√©s\n",
    "df['Tokens lemmatis√©s'] = df['Tokens'].apply(lemm2)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Repr√©sentation des mots\n",
    "\n",
    "Maintenant, visualisons pour certains commentaires les mots les plus repr√©sent√©s apr√®s nettoyage des commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commen√ßons d'abord par visualiser un premier nuage de mots d'un commentaire quelconque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui prend en argument une liste de tokens et qui retourne le nuage de mots correspondant.\n",
    "\n",
    "def cloud(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cloud(df['Tokens lemmatis√©s'][0]), interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, pour le premier commentaire de la bdd, on peut d√®s √† pr√©sent avoir une id√©e sur son avis vis √† vis du film gr√¢ce √† ce nuage de mots, de par la pr√©sence de mots tels que 'masterpiece, 'magnificent', 'perfection'... qui sont assez repr√©sent√©s. N√©anmoins, comme nous avons utilis√© nltk, le lemmatiseur ne reconnait pas les entit√©s nomm√©s telles que 'IMAX', 'India'... . \n",
    "La pr√©sence des mots concernants le temps comme \"waiting\", \"time\", \"longest\" nous indique que la dur√©e du film est un sujet des commentaires et nous pourrons voir par la suite s'il existe une corr√©lation entre la dur√©e du film et la polarit√© des commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** NLP et analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps nous allons appliquer les algorithmes de NLP sur les commentaires afin d'obtenir un score pour chaque commentaires puis nous allons modifier le dataset de telle sorte qu'une observation est un film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1** application d'algorithmes d'analyses de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertit les commentaires en string.\n",
    "def string_function(column):\n",
    "    return column.str.lower()\n",
    "\n",
    "df['Commentaire'] =  string_function(df[\"Commentaire\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons comparer les diff√©rents algorithmes propos√©s, notamment ceux des modules TextBlob, nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va dans un premier temps utiliser le module TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_blob(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applique la fonction √† la colonne 'tokens_lemmatized' du DataFrame\n",
    "df['sentiment_polarity_blob'] = df['Tokens lemmatis√©s'].apply(analyze_sentiment_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaye maintenant avec la fonction SentimentIntensityAnalyzer de nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_nltk(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['sentiment_polarity_nltk'] = df[\"Tokens lemmatis√©s\"].apply(analyze_sentiment_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessus prend environ 2 min √† s'ex√©cuter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2** manipulation pour la cr√©ation du nouveau dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables sont sous forme de string. Nous changeons cela pour pourvoir faire des statistiques avec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change le type de la colonne 'Note imdb' en float\n",
    "df['Note imdb'] = df['Note imdb'].str.replace(',', '.').astype(float)\n",
    "df['Annee'] = df['Annee'].astype(float)\n",
    "df[\"Budget\"] = df[\"Budget\"].astype(float)\n",
    "df['Box office'] = df[\"Box office\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On groupe les observations selon le film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('Film')\n",
    "df_grouped = grouped[[\"sentiment_polarity_nltk\", \"sentiment_polarity_blob\",  'Note imdb', 'Annee', \"Budget\", \"Box office\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Int√©ressons nous √† la corr√©lation des notes avec les scores moyens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va √©galement ajouter la dur√©e de chaque film dans le nouveau df \"df_grouped\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir une fonction pour s√©lectionner une observation pour chaque groupe\n",
    "def select_observation(group):\n",
    "    # Vous pouvez personnaliser cette fonction pour choisir une observation sp√©cifique,\n",
    "    # par exemple, ici, nous choisissons la premi√®re observation de chaque groupe.\n",
    "    return group.iloc[0]\n",
    "\n",
    "# Appliquer la fonction √† chaque groupe\n",
    "duration_serie = grouped['duree'].apply(select_observation)\n",
    "duration_serie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(duration_serie, df_grouped , on='Film', how='inner')\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il peut √™tre interessant de regarder aussi la variance des scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_by_group = grouped[[\"sentiment_polarity_nltk\", \"sentiment_polarity_blob\"]].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Join\" var_by_group et le df pour ajouter les variables de variance\n",
    "df_grouped_2 = pd.merge(var_by_group, df_grouped , on='Film', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Renomme les variables obtenues:\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_nltk_x': 'var_score_nltk'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_blob_x': 'var_score_blob'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_nltk_y': 'sentiment_polarity_nltk'}, inplace=True)\n",
    "df_grouped_2.rename(columns={'sentiment_polarity_blob_y': 'sentiment_polarity_blob'}, inplace=True)\n",
    "\n",
    "# R√©tablir l'index par d√©faut\n",
    "df_grouped_2.reset_index(inplace=True)\n",
    "print(df_grouped_2.shape)\n",
    "df_grouped_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_2.to_csv('data_post_process_2o2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_2 = pd.read_csv('data_post_process_2o2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v√©rification\n",
    "df_grouped_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos data ont √©t√© trait√©, nous allons proc√©der √† l'analyse statistique sur un autre notebook (\"Analyse_modelisation_3o3\") avec les \"data_post_process_2o2\" afin de faciliter la lecture du projet. En effet, l'ex√©cution de ce notebook peut prendre une quinzaine de minute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
